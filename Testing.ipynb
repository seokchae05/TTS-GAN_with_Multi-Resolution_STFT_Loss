{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Utils\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 로드\n",
    " - 데이터 셋 로드 및 데이터 로더 만들기\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=[512,512]),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5, ))\n",
    "])\n",
    "\n",
    "root_path = os.path.join(os.getcwd(), 'data')\n",
    "dataset = Utils.vibrationData(root_path=root_path, transform=transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, wavlet_img, corr_img, cls = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN ENCODER NETWORK\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, isize, nz, nc, ndf, ngpu, n_extra_layers=0, add_final_conv=True):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            isize (int): input image size\n",
    "            nz (int): size of the latent z vector\n",
    "            nc (int): input image channels\n",
    "            ndf (int): _description_\n",
    "            ngpu (int): number of GPUs to use\n",
    "            n_extra_layers (int, optional): number of layers on gen and disc. Defaults to 0.\n",
    "            add_final_conv (bool, optional): _description_. Defaults to True.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial-conv-{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 3, 2, 1, bias=False))\n",
    "        main.add_module('initial-relu-{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}-{1}-conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}-{1}-batchnorm'.format(t, cndf),\n",
    "                            nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}-{1}-relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid-{0}-{1}-conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 3, 2, 1, bias=False))\n",
    "            main.add_module('pyramid-{0}-batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid-{0}-relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        if add_final_conv:\n",
    "            main.add_module('final-{0}-{1}-conv'.format(cndf, 1),\n",
    "                            nn.Conv2d(cndf, nz, 3, 1, 0, bias=False))\n",
    "\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN DECODER NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, isize, nz, nc, ngf, ngpu, n_extra_layers=0):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            isize (int): input image size\n",
    "            nz (int): size of the latent z vector\n",
    "            nc (int): input image channels\n",
    "            ngf (_type_): _description_\n",
    "            ngpu (int): number of GPUs to use\n",
    "            n_extra_layers (int, optional): number of layers on gen and disc. Defaults to 0.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "        \n",
    "        cngf, tisize = ngf // 2, 4\n",
    "        \n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial-{0}-{1}-convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 3, 1, 0, bias=False))\n",
    "        main.add_module('initial-{0}-batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial-{0}-relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, _ = 4, cngf\n",
    "        while csize < isize // 2:\n",
    "            main.add_module('pyramid-{0}-{1}-convt'.format(cngf, cngf // 2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf // 2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid-{0}-batchnorm'.format(cngf // 2),\n",
    "                            nn.BatchNorm2d(cngf // 2))\n",
    "            main.add_module('pyramid-{0}-relu'.format(cngf // 2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}-{1}-conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}-{1}-batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}-{1}-relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final-{0}-{1}-convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final-{0}-tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(isize=512, nz=512, nc=4, ndf=64, ngpu=0)\n",
    "z = encoder(wavlet_img)\n",
    "\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_cls, isize, nz, nc, ngf, ngpu, n_extra_layers=0):\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_cls = n_cls\n",
    "        self.pre_z = nn.Sequential(\n",
    "            nn.Linear(2048, 3*nz),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.pre_c = nn.Sequential(\n",
    "            nn.Linear(n_cls, 1*nz),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.layers = Decoder(isize=isize, nz=nz, nc=nc, ngf=ngf, ngpu=ngpu, n_extra_layers=n_extra_layers)\n",
    "        \n",
    "    def forward(self, z, c):\n",
    "        \n",
    "        batch_size = z.size(0)\n",
    "        \n",
    "        z = z.view([batch_size, -1])\n",
    "        z = self.pre_z(z)\n",
    "        z = z.view([batch_size, -1, 2, 2])\n",
    "        c = self.pre_c(c)\n",
    "        c = c.view([batch_size, -1, 2, 2])\n",
    "        \n",
    "        zc = torch.cat([z,c], dim=1)\n",
    "\n",
    "        return self.layers(zc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(n_cls =2, isize=512, nz=512, nc=4, ngf=64, ngpu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 2, 2])\n",
      "torch.Size([2, 4, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "rand_z = 2 * torch.rand(wavlet_img.size(0), 512, 2, 2) - 1\n",
    "x = generator(rand_z, cls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,n_cls=2, isize=512, nz=100, nc=4, ngf=64, ngpu=0):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.pre_x = Encoder(isize=512, nz=100, nc=4, ndf=64, ngpu=0)\n",
    "        # 2x2x100\n",
    "        \n",
    "        self.pre_z = nn.Sequential(\n",
    "            nn.Linear(2048, 3*nz),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # flatten(2x2x75)\n",
    "        \n",
    "        \n",
    "        self.pre_c = nn.Sequential(\n",
    "            nn.Linear(n_cls, 1*nz),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # flatten(2x2x25)\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2448, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "        \n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x, z, c):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.pre_x(x)\n",
    "        z = z.view(batch_size, -1)\n",
    "\n",
    "        z = self.pre_z(z)\n",
    "        \n",
    "        \n",
    "        \n",
    "        z = z.view([batch_size, -1, 2, 2])\n",
    "        c = self.pre_c(c)\n",
    "        c = c.view([batch_size, -1, 2, 2])\n",
    "        \n",
    "        zc = torch.cat([z,c], dim=1)\n",
    "        xzc = torch.cat([x,zc], dim=1)\n",
    "        \n",
    "        # 2x2x200\n",
    "        xzc = xzc.view(batch_size, -1)\n",
    "        \n",
    "        result = self.layers(xzc)\n",
    "        \n",
    "    \n",
    "        return result\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : torch.Size([2, 4, 512, 512]), z : torch.Size([2, 512, 2, 2]), cls : torch.Size([2, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x800 and 2448x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mx : \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m, z : \u001b[39m\u001b[39m{\u001b[39;00mz\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m, cls : \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m dis(x, z, \u001b[39mcls\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Vibration/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[64], line 63\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x, z, c)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m# 2x2x200\u001b[39;00m\n\u001b[1;32m     61\u001b[0m xzc \u001b[39m=\u001b[39m xzc\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(xzc)\n\u001b[1;32m     66\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Vibration/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Vibration/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Vibration/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Vibration/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x800 and 2448x1024)"
     ]
    }
   ],
   "source": [
    "# print(f'x : {x.size()}, z : {z.size()}, cls : {cls.size()}')\n",
    "\n",
    "# dis(x, z, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_loss(DG, DE, eps=1e-6):\n",
    "    loss = torch.log(DE + eps) + torch.log(1 - DG + eps)\n",
    "    return -torch.mean(loss)\n",
    "\n",
    "\n",
    "def EG_loss(DG, DE, eps=1e-6):\n",
    "    loss = torch.log(DG + eps) + torch.log(1 - DE + eps)\n",
    "    return -torch.mean(loss)\n",
    "\n",
    "def initialize_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(model.bias.data,0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "l_rate = 2e-5\n",
    "\n",
    "E = Encoder(isize=512, nz=512, nc=4, ndf=64, ngpu=0)\n",
    "G = Generator(n_cls =2, isize=512, nz=512, nc=4, ngf=64, ngpu=0)\n",
    "D = Discriminator(n_cls=2, isize=512, nz=512, nc=4, ngf=64, ngpu=0)\n",
    "\n",
    "E.apply(initialize_weights)\n",
    "G.apply(initialize_weights)\n",
    "D.apply(initialize_weights)\n",
    "\n",
    "optimizer_EG = torch.optim.Adam(list(E.parameters()) + list(G.parameters()), \n",
    "                                lr=l_rate, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=l_rate, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image : torch.Size([2, 4, 512, 512]), Gz : torch.Size([2, 4, 512, 512])\n",
      "rand_z : torch.Size([2, 512, 2, 2]), Ex : torch.Size([2, 512, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:14, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m#Discriminator training\u001b[39;00m\n\u001b[1;32m     45\u001b[0m optimizer_D\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 46\u001b[0m loss_D\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     47\u001b[0m optimizer_D\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Vibration/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Vibration/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cpu'\n",
    "for epoch in range(n_epochs):\n",
    "    D_loss_acc = 0.\n",
    "    EG_loss_acc = 0.\n",
    "    D.train()\n",
    "    E.train()\n",
    "    G.train()\n",
    "        \n",
    "#     scheduler_D.step()\n",
    "#     scheduler_EG.step()\n",
    "    \n",
    "    for i, (signal, images, corr_img, c) in tqdm(enumerate(data_loader)):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        \n",
    "        rand_z = 2 * torch.rand(wavlet_img.size(0), 512, 2, 2) - 1\n",
    "        rand_z = rand_z.to(device)\n",
    "        \n",
    "        #compute G(z, c) and E(X)\n",
    "        \n",
    "        Gz = G(rand_z, c)\n",
    "        EX = E(images)\n",
    "        \n",
    "        print(f'image : {images.size()}, Gz : {Gz.size()}')\n",
    "        print(f'rand_z : {rand_z.size()}, Ex : {EX.size()}')\n",
    "        DE = D(images, EX, c)\n",
    "        DG = D(Gz, rand_z, c)\n",
    "        \n",
    "        \n",
    "        loss_EG = EG_loss(DG, DE)\n",
    "        #Encoder & Generator training\n",
    "        optimizer_EG.zero_grad()\n",
    "        loss_EG.backward()\n",
    "        optimizer_EG.step()\n",
    "        #compute D(G(z, c), z, c) and D(X, E(X), c)\n",
    "        #compute losses\n",
    "        loss_D = D_loss(DG, DE)\n",
    "        \n",
    "        D_loss_acc += loss_D.item()\n",
    "        EG_loss_acc += loss_EG.item()\n",
    "        \n",
    "        #Discriminator training\n",
    "        optimizer_D.zero_grad()\n",
    "        loss_D.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Avg_Loss_D: {:.4f}, Avg_Loss_EG: {:.4f}'\n",
    "                    .format(epoch + 1, n_epochs, D_loss_acc / i, EG_loss_acc / i))\n",
    "            n_show = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vibration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7d95161f7bedbfa09496c29856a74c524c6bfebe9bd569c5b5b43dec9adf50b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
